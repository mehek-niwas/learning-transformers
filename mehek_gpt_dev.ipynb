{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMH/tufHv1cI15OHQckDug6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehek-niwas/learning_transformer/blob/main/mehek_gpt_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NanoTransformer Notes**\n",
        "*using Andrej Karpathy's video*: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1334s"
      ],
      "metadata": {
        "id": "EtqtfnAwNUM8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SEctH165761",
        "outputId": "b8ba4dcc-6748-46db-fc97-b7a25f369f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-09 21:13:30--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-01-09 21:13:30 (130 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3lCF_WB6ODO",
        "outputId": "b3ecb8c0-3968-4e7e-8c61-584a620c5982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afu3C2G66ZnE",
        "outputId": "cff6c0a4-ff86-4ae6-b54d-98694700d96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou0Y0CID6iFw",
        "outputId": "dc86a595-df22-4262-fd26-f5a499608841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSLDwu_66tos",
        "outputId": "fd3ec5d1-ef97-47a0-a0bc-d0bf4e5a4ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "ogK8qKSr8Xb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"training dataset: \", train_data.shape)\n",
        "print(\"validation dataset: \", val_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8iX7K-T8f06",
        "outputId": "fdf7bf43-68c5-4ea6-ac43-2de63fec9071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training dataset:  torch.Size([1003854])\n",
            "validation dataset:  torch.Size([111540])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1] # showing first block (batch) --> represents 8 training samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdsaCKwm8bNf",
        "outputId": "5791bcfb-f97a-4704-9b84-0c2e4d4831d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all from the first batch\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K72phDTV-ZGy",
        "outputId": "9201145c-9199-4be1-a8b8-a2714f0905b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"training dataset: \", train_data.shape)\n",
        "print(\"validation dataset: \", val_data.shape)\n",
        "\n",
        "torch.manual_seed(1337) # random number generator for reproducable code\n",
        "\n",
        "batch_size = 4 # ---> THIS IS THE NUMBER OF SEQUENCES PER BATCH\n",
        "block_size = 8 # ---> (MAXIMUM TOKENS PER BATCH PER SEQUENCE) or maximum context length\n",
        "\n",
        "# disclaimer: this code does not ensure block_size seperation in between starting indicies\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    # if in the train split, we are going to look at the training data\n",
        "    # if in test split, we are going to look at the testing data\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # generate (batch_size) random starting indexes anywhere from 0 to data.length - block_size\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # torch.randint(low=0, high, size) --> where size is tuple (rows, columns) --> for rows, 1 column --> (rows,)\n",
        "\n",
        "    # create batches from random starting indexes\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEKNAi6cB2fK",
        "outputId": "b61245c3-1cd6-4148-d95b-d8ce28eddb41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training dataset:  torch.Size([1003854])\n",
            "validation dataset:  torch.Size([111540])\n",
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# BigramLanguageModel inherits from nn.Module\n",
        "# nn.Module --> base class for all neural network modules. user models should also subclass the Module class\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  # constructor\n",
        "  def __init__(self, vocab_size): #\n",
        "    # ensures the parents class (nn.Module) is properly initialized before adding additional functionality specific to the BigramLanguageModel\n",
        "\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "    # SYNTAX: nn.Embedding(num_embeddings, embedding_dim) --> (size of dictionary of embeddings, size of each embedding vector)\n",
        "    #  --> OUTPUT:  random initialized embedding matrix of vocab_size x vocab_size\n",
        "\n",
        "  def forward(self, inputs, targets=None):\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "\n",
        "    # THIS WORKS FOR THE INPUTS BECAUSE THE INPUTS ARE ALREADY IN INDEX FORM. SO INPUT OF 23 WILL OUTPUT THE EMBEDDING ROW 23\n",
        "    # THIS WOULD NOT WORK IF THE INPUTS WERE STILL IN STRING FORM. --> WE HAD TO SPLIT THEM AND INDEX THEM FIRST(SERVING A LOOKUP TABLE OR WHATEVER METHOD) --> SO NOW WE ARE SWITCHING TO ITS EMBEDDING VECTOR\n",
        "\n",
        "\n",
        "    logits = self.token_embedding_table(inputs) # --> WE NEED TO ARRANGE THIS AS A (B, T, C) # ==== the tensors/embedding vectors related to the idx(s) [from the embedding matrix]\n",
        "    # B = batch size (number of input sequences)  x   T = sequence length  x  C = vocabulary size (logits for each possible token??) --> logits --> interpreted as the scores for the next character in the sequence\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "    # --> SO pytorch expects a B, C, T matrix as the input if the input is multidimensional (btc = 4x8x65)\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C) # making it into a 2 dimensional array!! (was previously B,T,C )\n",
        "      targets = targets.view(B*T) # making into 1 dimensional array!! (was previously B,T)\n",
        "      loss = F.cross_entropy(logits, targets) # loss is the cross entropy of the logits and targets. measures the quality of the logits with respect to the targets\n",
        "      # but targets is not in 4 x 8 x 65 format? it is just in 4 x 8 format?\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx) # --> self(idx) goes to the forward function --> self.function(idx)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) # AUTOREGRESSIVE PART OF THE BIGRAM MODEL!!\n",
        "\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size) # vocab size is the number of different characters in the input\n",
        "logits, loss = m(xb, yb) # same as m.forward(xb, yb) because PyTorch automatically directs m(xb, yb) to the forward method\n",
        "# ---> the scores/logits of a probabilities of each of the 65 character being the next part of the sequence (for every one of the 4 x 8 positions).  ---> 4 x 8 x 65 (imagine a very dense cube)\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "inputs = torch.zeros((1, 1), dtype=torch.long) # zero is kind of like the SOS token since it is the newline character for this project\n",
        "print(decode(m.generate(inputs, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "ew2HauMBU2jy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "743da3be-fff9-4b35-895a-34a734373ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'xb' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3a151d9182b1>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigramLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# vocab size is the number of different characters in the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# same as m.forward(xb, yb) because PyTorch automatically directs m(xb, yb) to the forward method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;31m# ---> the scores/logits of a probabilities of each of the 65 character being the next part of the sequence (for every one of the 4 x 8 positions).  ---> 4 x 8 x 65 (imagine a very dense cube)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "think of \"T\" in the BTC as the \"time dimension\" or the \"time step\""
      ],
      "metadata": {
        "id": "7W9CfLCSuf6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = \"blue\"> remember the target in transformers is ALWAYS only 1 token\n",
        "\n",
        "# <font color = \"blue\"> the input in transformers can be a RANGE of tokens up to the CONTEXT WINDOW/SIZE"
      ],
      "metadata": {
        "id": "VMN1ZHlgvLxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PyTorch Embedding Notes**\n",
        "*key component in transformers. used to convert input tokens into continuous representations*\n",
        "\n",
        "\"an embedding layer is a simple lookup table that stores embeddings of a fixed dictionary size\"\n",
        "- the embedding layer = lookup table that maps an index value to a weight matrix of (user-defined) dimension\n",
        "- weight matrix is optimized to produce more useful vectors\n",
        "--> embedding matrix is initialized: `num_embeddings x embedding_dim`\n",
        "- NUM_EMBEDDINGS IS THE DICTIONARY SIZE\n",
        "- given input word or token... (represented by index in the vocabulary)... the index is passed to the embedding layer which looks up the corresponding row in the embedding matrix\n"
      ],
      "metadata": {
        "id": "MPIaR6arMIqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "ekiuFT73xDpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "k7pZxnKZxbFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etagk2TaxHYI",
        "outputId": "89437308-b184-4847-969e-1b99403960c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5580098628997803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJQJg286xJkH",
        "outputId": "ffd3d607-8677-4a96-d311-039e1db19622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bur y ang wod, t se atperares, m:\n",
            "Frerf y g, ncorparthe owns?\n",
            "\n",
            "Tire\n",
            "I trd odeloutes, e couklonsthendf.\n",
            "PESh thovenofieaye kellar we gr mpanou s this?\n",
            "\n",
            "Noflp\n",
            "Ron:\n",
            "OME trdovy\n",
            "IO:\n",
            "Or so waco is. he s!\n",
            "CI I w'd Pordervet ld, temofostrurd tiomy Proute we g gary outhend le, le, yed rat y ay m.\n",
            "\n",
            "nd, yorix$Jave ed thais, mea l herirnonais ire mo par,\n",
            "OFOnealle qu f t atary meee dd il thy fitito-the ssd ovel y hathe, qun s;\n",
            "LO tond t, oullke CIARicousangh'dlke, y lorveloveninde searslly otharomeligse ang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Check\n"
      ],
      "metadata": {
        "id": "z-ZVA1XclsxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi # making sure GPU is running"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JcmW1U2lvXM",
        "outputId": "afd196e9-646e-4142-e28a-f57193710aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan  9 21:14:10 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "numGPUs = len(tf.config.experimental.list_physical_devices('GPU'))\n",
        "\n",
        "print('Num GPUs Available: ', numGPUs)\n",
        "\n",
        "if numGPUs > 0:\n",
        "  print(tf.test.gpu_device_name())\n",
        "  print(device_lib.list_local_devices()[1].physical_device_desc)\n",
        "\n",
        "# check if cuda is available\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f34vfNJFl5IW",
        "outputId": "0abd610e-fb23-4324-e30f-5c9834561bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "/device:GPU:0\n",
            "device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "CUDA is available!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script-Version"
      ],
      "metadata": {
        "id": "dUaw4uYTclz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    # model.eval() is important for disabling droupout layers and adjusting batch normalization behavior\n",
        "    # model.eval() does not automatically disable local gradient computation\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvKN3KjPcksz",
        "outputId": "a8d51bc2-42ab-4895-93b9-285e1f0607fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-09 21:14:19--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.005s  \n",
            "\n",
            "2025-01-09 21:14:19 (209 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n",
            "step 0: train loss 4.7305, val loss 4.7241\n",
            "step 300: train loss 2.8110, val loss 2.8249\n",
            "step 600: train loss 2.5434, val loss 2.5682\n",
            "step 900: train loss 2.4932, val loss 2.5088\n",
            "step 1200: train loss 2.4863, val loss 2.5035\n",
            "step 1500: train loss 2.4665, val loss 2.4921\n",
            "step 1800: train loss 2.4683, val loss 2.4936\n",
            "step 2100: train loss 2.4696, val loss 2.4846\n",
            "step 2400: train loss 2.4638, val loss 2.4879\n",
            "step 2700: train loss 2.4738, val loss 2.4911\n",
            "\n",
            "\n",
            "\n",
            "CEThik brid owindakis b, bth\n",
            "\n",
            "HAPet bobe d e.\n",
            "S:\n",
            "O:3 my d?\n",
            "LUCous:\n",
            "Wanthar u qur, t.\n",
            "War dXENDoate awice my.\n",
            "\n",
            "Hastarom oroup\n",
            "Yowhthetof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghir.\n",
            "Swanousel lind me l.\n",
            "HAshe ce hiry:\n",
            "Supr aisspllw y.\n",
            "Hentofu n Boopetelaves\n",
            "MPOLI s, d mothakleo Windo whth eisbyo the m dourive we higend t so mower; te\n",
            "\n",
            "AN ad nterupt f s ar igr t m:\n",
            "\n",
            "Thin maleronth,\n",
            "Mad\n",
            "RD:\n",
            "\n",
            "WISo myrangoube!\n",
            "KENob&y, wardsal thes ghesthinin couk ay aney IOUSts I&fr y ce.\n",
            "J\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"The mathematical trick in self-attention\""
      ],
      "metadata": {
        "id": "ttZUEK4xmLWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time (max sequence/context length), channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b42PINVQmPji",
        "outputId": "d8c50827-a384-41e3-c2ca-320a85c98f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4tn5EQ5mkvE",
        "outputId": "31326f41-587b-4593-c151-e7a5bf5d808d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we want the 8 tokens (T = 6) to be able to talk to each other\n",
        "\n",
        "the tokens should not be able to talk to tokens in the future\n",
        "\n",
        "information only flows from the previous context to the current time step. and we can't get any information from the future\n",
        "\n",
        "so if i was token 5, id want all the channels from token time steps of 4, 3, 2, and 1 ---> averaging them up --> so that it becomes like a feature vector that sort of summarizes me in the context of my history. just an average will be lossy, so we can do some more special stuff later\n"
      ],
      "metadata": {
        "id": "3ZjyjQjjmzjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An inefficient way to get averages of the previous tokens"
      ],
      "metadata": {
        "id": "kcnSwz1viRBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1] # size: (1, t, C) ---> so technically (t, C) since u can ignore 1 at that point\n",
        "    xbow[b,t] = torch.mean(xprev, 0) # dim=0 means that the mean is computed along the \"first axis\"( --> meaning rows) of the tensor\n",
        "    # so for each column in xprev, the function calculates the mean of all rows (time steps)\n",
        "    # torch.mean(xprev, 0) --> averaging each time step by channel --> (C, )"
      ],
      "metadata": {
        "id": "8INscWeeoB2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[0])\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ2P531zLdEj",
        "outputId": "1459bcc7-7de6-4971-acb7-def33f488361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "torch.Size([4, 8, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJyKkZTbL9yk",
        "outputId": "95d170a1-c78b-4323-de3f-a2a83815b0eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write code that prints which inputs in x that a row of x bow is an average of\n",
        "for minibatch in range(B):\n",
        "  print(\"***********************minibatch start******************************\")\n",
        "  for tokenStep in range(T):\n",
        "    print(xbow[minibatch, tokenStep])\n",
        "    print(\"is an average of: \")\n",
        "    print(x[minibatch, :tokenStep+1])\n",
        "    print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YndpA0taMKk6",
        "outputId": "503150b3-0650-484b-b819-2f2a0906aad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********************minibatch start******************************\n",
            "tensor([ 0.1808, -0.0700])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.0894, -0.4926])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.1490, -0.3199])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.3504, -0.2238])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643]])\n",
            "--------------------------------------------------\n",
            "tensor([0.3525, 0.0545])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.0688, -0.0396])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.0927, -0.0682])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.0341,  0.1332])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "--------------------------------------------------\n",
            "***********************minibatch start******************************\n",
            "tensor([ 1.3488, -0.1396])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396]])\n",
            "--------------------------------------------------\n",
            "tensor([0.8173, 0.4127])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.1342,  0.4395])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931]])\n",
            "--------------------------------------------------\n",
            "tensor([0.2711, 0.4774])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910]])\n",
            "--------------------------------------------------\n",
            "tensor([0.2421, 0.0694])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910],\n",
            "        [ 0.1260, -1.5627]])\n",
            "--------------------------------------------------\n",
            "tensor([0.0084, 0.0020])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910],\n",
            "        [ 0.1260, -1.5627],\n",
            "        [-1.1601, -0.3348]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.0712, -0.1128])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910],\n",
            "        [ 0.1260, -1.5627],\n",
            "        [-1.1601, -0.3348],\n",
            "        [ 0.4478, -0.8016]])\n",
            "--------------------------------------------------\n",
            "tensor([0.2527, 0.2149])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910],\n",
            "        [ 0.1260, -1.5627],\n",
            "        [-1.1601, -0.3348],\n",
            "        [ 0.4478, -0.8016],\n",
            "        [ 1.5236,  2.5086]])\n",
            "--------------------------------------------------\n",
            "***********************minibatch start******************************\n",
            "tensor([-0.6631, -0.2513])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.1735, -0.0649])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215]])\n",
            "--------------------------------------------------\n",
            "tensor([0.1685, 0.3348])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.1621,  0.1765])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340],\n",
            "        [-1.1539, -0.2984]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.2312, -0.0436])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340],\n",
            "        [-1.1539, -0.2984],\n",
            "        [-0.5075, -0.9239]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.1015, -0.2855])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340],\n",
            "        [-1.1539, -0.2984],\n",
            "        [-0.5075, -0.9239],\n",
            "        [ 0.5467, -1.4948]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.2593, -0.1630])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340],\n",
            "        [-1.1539, -0.2984],\n",
            "        [-0.5075, -0.9239],\n",
            "        [ 0.5467, -1.4948],\n",
            "        [-1.2057,  0.5718]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.3015, -0.2293])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340],\n",
            "        [-1.1539, -0.2984],\n",
            "        [-0.5075, -0.9239],\n",
            "        [ 0.5467, -1.4948],\n",
            "        [-1.2057,  0.5718],\n",
            "        [-0.5974, -0.6937]])\n",
            "--------------------------------------------------\n",
            "***********************minibatch start******************************\n",
            "tensor([ 1.6455, -0.8030])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.4985, -0.5395])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759]])\n",
            "--------------------------------------------------\n",
            "tensor([0.4954, 0.3420])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.0623, -0.1802])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048],\n",
            "        [ 2.7630, -1.7465]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.1401, -0.4462])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048],\n",
            "        [ 2.7630, -1.7465],\n",
            "        [ 1.4516, -1.5103]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.0870, -0.4071])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048],\n",
            "        [ 2.7630, -1.7465],\n",
            "        [ 1.4516, -1.5103],\n",
            "        [ 0.8212, -0.2115]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.0430, -0.1299])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048],\n",
            "        [ 2.7630, -1.7465],\n",
            "        [ 1.4516, -1.5103],\n",
            "        [ 0.8212, -0.2115],\n",
            "        [ 0.7789,  1.5333]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.1138, -0.1641])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048],\n",
            "        [ 2.7630, -1.7465],\n",
            "        [ 1.4516, -1.5103],\n",
            "        [ 0.8212, -0.2115],\n",
            "        [ 0.7789,  1.5333],\n",
            "        [ 1.6097, -0.4032]])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An efficient way to get averages of the previous tokens"
      ],
      "metadata": {
        "id": "3nElV1Axn36S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T,T))\n",
        "print(wei)\n",
        "print(wei.sum(1, keepdim = True))\n",
        "# NORMALIZING THE MATRIX\n",
        "wei = wei / wei.sum(1, keepdim=True) # wei / (summing across columns --> so sum each row) = average of\n",
        "print(wei)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvI8OPIoj8GF",
        "outputId": "05a959a8-ca6f-49bb-f76f-e4d625f87f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [4.],\n",
            "        [5.],\n",
            "        [6.],\n",
            "        [7.],\n",
            "        [8.]])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# My method\n",
        "for minibatch in range(B):\n",
        "  currX = x[minibatch]\n",
        "  print(wei @ currX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdB_Ug-dm9Sq",
        "outputId": "e427ed7c-7a68-4e5c-be81-a8f8e0db5a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.8173,  0.4127],\n",
            "        [-0.1342,  0.4395],\n",
            "        [ 0.2711,  0.4774],\n",
            "        [ 0.2421,  0.0694],\n",
            "        [ 0.0084,  0.0020],\n",
            "        [ 0.0712, -0.1128],\n",
            "        [ 0.2527,  0.2149]])\n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 0.1735, -0.0649],\n",
            "        [ 0.1685,  0.3348],\n",
            "        [-0.1621,  0.1765],\n",
            "        [-0.2312, -0.0436],\n",
            "        [-0.1015, -0.2855],\n",
            "        [-0.2593, -0.1630],\n",
            "        [-0.3015, -0.2293]])\n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.4985, -0.5395],\n",
            "        [ 0.4954,  0.3420],\n",
            "        [ 1.0623, -0.1802],\n",
            "        [ 1.1401, -0.4462],\n",
            "        [ 1.0870, -0.4071],\n",
            "        [ 1.0430, -0.1299],\n",
            "        [ 1.1138, -0.1641]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch method\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "print(xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeoJBd0LoKIp",
        "outputId": "e2951e04-8e2f-46d4-e10a-988a2ab67db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An even more efficient way to get averages of the previous tokens"
      ],
      "metadata": {
        "id": "JqkzpK9LrIgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T,T))\n",
        "print(tril)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZuHmbPl_hsw",
        "outputId": "6cecb807-ed59-4d40-8acc-b3eefb533faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.zeros((T,T))\n",
        "print(wei)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # make all elements where tril = 0, --> replace with -inf\n",
        "print(wei)\n",
        "wei = F.softmax(wei, dim=-1) # dim=-1 --> means take softmax along every single row\n",
        "print(wei)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdZ0aI_tAn1n",
        "outputId": "afb4e026-970f-409b-beb8-45763db0617c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN0-fMfKMGlv",
        "outputId": "270bff32-0c5d-450f-9aac-4143f8614e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### in attention, we use weights similar to the above matrix. the mask will remain, but instead of normalized average weights, the weights will be based on the amount of affinity a token has for the next token prediction.\n",
        "\n",
        "### when you multiply the weights by the token embeddings, you are multiply the weights by each channel to get a token embeddings matrix. so the same weight will be applied all of the channels for a certain sample (feature-vector)\n",
        "\n"
      ],
      "metadata": {
        "id": "Js4L5_vrLrxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHFcVGE5MDJH",
        "outputId": "ad3d9202-2480-4341-e25a-f823e1c5dc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.3596, -0.9152],\n",
              "         [ 0.6258,  0.0255],\n",
              "         [ 0.9545,  0.0643],\n",
              "         [ 0.3612,  1.1679],\n",
              "         [-1.3499, -0.5102],\n",
              "         [ 0.2360, -0.2398],\n",
              "         [-0.9211,  1.5433]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.2858,  0.9651],\n",
              "         [-2.0371,  0.4931],\n",
              "         [ 1.4870,  0.5910],\n",
              "         [ 0.1260, -1.5627],\n",
              "         [-1.1601, -0.3348],\n",
              "         [ 0.4478, -0.8016],\n",
              "         [ 1.5236,  2.5086]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 1.0101,  0.1215],\n",
              "         [ 0.1584,  1.1340],\n",
              "         [-1.1539, -0.2984],\n",
              "         [-0.5075, -0.9239],\n",
              "         [ 0.5467, -1.4948],\n",
              "         [-1.2057,  0.5718],\n",
              "         [-0.5974, -0.6937]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.3514, -0.2759],\n",
              "         [-1.5108,  2.1048],\n",
              "         [ 2.7630, -1.7465],\n",
              "         [ 1.4516, -1.5103],\n",
              "         [ 0.8212, -0.2115],\n",
              "         [ 0.7789,  1.5333],\n",
              "         [ 1.6097, -0.4032]]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei @ x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_dhJ3ohAwk7",
        "outputId": "efbcd13d-adcc-47ea-a6c6-13115e795958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 0.1735, -0.0649],\n",
              "         [ 0.1685,  0.3348],\n",
              "         [-0.1621,  0.1765],\n",
              "         [-0.2312, -0.0436],\n",
              "         [-0.1015, -0.2855],\n",
              "         [-0.2593, -0.1630],\n",
              "         [-0.3015, -0.2293]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.4985, -0.5395],\n",
              "         [ 0.4954,  0.3420],\n",
              "         [ 1.0623, -0.1802],\n",
              "         [ 1.1401, -0.4462],\n",
              "         [ 1.0870, -0.4071],\n",
              "         [ 1.0430, -0.1299],\n",
              "         [ 1.1138, -0.1641]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # since -inf = 0 in softmmax\n",
        "wei = F.softmax(wei, dim=-1) # ---> softmax is a normalization function\n",
        "xbow3 = wei @ x\n",
        "print(xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ei5eEjbrKqT",
        "outputId": "742038bb-c045-4f6e-d1aa-737d48d9eaac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The self attention way"
      ],
      "metadata": {
        "id": "cNwGTnYbFWdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# single head of attention\n",
        "head_size = 16\n",
        "\n",
        "key = nn.Linear(C, head_size, bias=False) # linear layer to convert each token embedding into a lower dimension embedding (dim = head_size) --> key *embedding* vector\n",
        "# ^^ so (B, T, embed_dim) will be able to become (B, T, head_size) = KEY\n",
        "\n",
        "query = nn.Linear(C, head_size, bias=False) # linear layer to convert each token embedding into a lower dimension embedding (dim = head_size) --> value *embedding* vector\n",
        "# ^^ (B, T, embed_dim) will be able to become (B, T, head_size) = QUERY\n",
        "\n",
        "value = nn.Linear(C, head_size, bias=False) # linear layer to convert each token embedding into a lower dimension embedding (dim = head_size) --> value *embedding* vector\n",
        "# ^^ (B, T, embed_dim) will be able to become (B, T, head_size) = VALUE\n",
        "\n",
        "k = key(x) # (B, T, head_size)\n",
        "q = query(x) # (B, T, head_size)\n",
        "\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, head_size) x (B, head_size, T) = (B, T, T) --> token_length x token_length x num_minibatches\n",
        "# WEI IS NO LONGER JUST ZEROS\n",
        "print(\"after qk dot product\")\n",
        "print(wei)\n",
        "print(\"\\n\")\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(\"after masking\")\n",
        "print(wei)\n",
        "print(\"\\n\")\n",
        "\n",
        "v = value(x) # (B, T, head_size) wei = q @ k.transpose(-2, -1) # (B, T, head_size) x (B, head_size, T) = (B, T, T) --> token_length x token_length x num_minibatches\n",
        "\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(\"after softmax\")\n",
        "print(wei)\n",
        "print(\"\\n\")\n",
        "\n",
        "out = wei @ v  # (B, T, T) x (B, T, head_size) = (B, T, head_size)\n",
        "print(\"after wei @ v\")\n",
        "print(out)\n",
        "print(\"\\n\")\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-J0Dlg2FfZm",
        "outputId": "6b062bd9-0b92-462a-c3f0-472e3301211a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after qk dot product\n",
            "tensor([[[-3.2516e-01,  7.5986e-02, -8.7015e-01,  1.3635e+00, -1.8130e-01,\n",
            "          -7.6186e-01, -1.9567e-01,  1.3711e+00],\n",
            "         [ 2.2350e-01,  3.7087e-01, -9.1976e-01,  1.2680e+00,  5.4802e-01,\n",
            "          -8.2141e-01, -3.2949e-01,  6.7529e-01],\n",
            "         [-3.5427e-01, -1.0413e+00,  1.5438e+00,  4.9150e-01, -2.7205e-01,\n",
            "           1.4554e+00, -3.2128e+00,  3.1941e-01],\n",
            "         [ 2.1174e-01, -2.9837e-01,  6.0922e-01, -1.4986e+00,  5.5533e-01,\n",
            "           1.7037e+00, -9.9851e-01, -2.4456e+00],\n",
            "         [-3.2672e-01, -1.9602e+00,  2.9103e-01, -7.8674e-01, -1.3949e+00,\n",
            "           1.2709e+00,  5.1601e-01,  3.3495e-01],\n",
            "         [ 1.7287e-01, -4.2323e-01, -5.1888e-01,  1.1365e+00,  2.2165e-01,\n",
            "           2.9291e-02, -3.1094e-01,  1.2146e+00],\n",
            "         [-1.8122e-01,  9.5256e-01, -2.1602e+00,  1.0454e-01, -5.0251e-01,\n",
            "           6.0112e-01,  4.0828e+00, -7.1607e-01],\n",
            "         [-7.4617e-01, -2.5025e-01,  1.1514e+00, -1.0002e-01, -1.8712e+00,\n",
            "           4.2339e-01, -1.0692e+00,  1.5323e-01]],\n",
            "\n",
            "        [[-5.5930e-01,  3.4945e-01,  1.8007e+00,  1.9388e+00, -1.3285e+00,\n",
            "           9.4139e-01,  2.0488e+00, -1.4216e+00],\n",
            "         [ 9.4424e-02, -7.2168e-01, -1.4134e+00,  5.0276e-01,  1.4706e-02,\n",
            "           1.2319e+00, -9.8834e-01,  4.8457e-01],\n",
            "         [ 1.0768e+00, -2.8335e-01, -2.4402e-01,  9.4388e-01,  3.0387e-01,\n",
            "           4.5961e-01,  5.5620e-01, -1.4221e+00],\n",
            "         [ 1.1073e+00,  1.0650e-01,  1.6771e-01, -2.1446e+00, -5.4226e-01,\n",
            "           1.2888e+00, -1.6738e-01,  1.0945e+00],\n",
            "         [ 2.1630e-01,  1.2713e+00, -5.8886e-01, -1.5853e+00, -8.7814e-01,\n",
            "           5.2919e-01, -1.3563e+00,  2.3625e+00],\n",
            "         [-7.3361e-01,  8.6075e-01,  1.4868e+00, -2.1331e-01, -9.3783e-01,\n",
            "          -1.9511e+00,  1.1838e+00,  1.4287e+00],\n",
            "         [ 8.9861e-01, -1.2177e+00,  4.1005e-01,  5.0886e-01, -4.8051e-01,\n",
            "           2.2417e-01,  1.2294e+00, -6.6410e-01],\n",
            "         [ 1.2787e-01, -6.8581e-01,  3.3694e-01, -3.7864e-01, -9.6183e-01,\n",
            "           4.9140e-01,  4.8948e-01,  1.8815e+00]],\n",
            "\n",
            "        [[-1.9182e+00,  7.9033e-01,  1.3331e+00, -2.6643e+00, -7.1192e-02,\n",
            "          -9.3979e-01, -9.2351e-01,  4.8031e-04],\n",
            "         [ 1.5381e+00, -3.0293e-01, -1.2885e+00,  1.4862e+00, -1.7103e+00,\n",
            "           2.5432e+00, -4.4183e-01,  4.0223e-01],\n",
            "         [ 2.3563e-01, -1.2836e+00,  5.4389e-01, -5.8813e-01,  3.5369e-01,\n",
            "          -5.7140e-02,  1.7754e-01,  4.9934e-01],\n",
            "         [-9.0606e-01,  5.4603e-01, -1.3593e+00, -1.6237e-01,  2.7268e+00,\n",
            "          -4.4452e-01, -1.1820e+00, -8.3098e-01],\n",
            "         [ 1.1861e-01,  1.7941e+00,  5.3429e-02, -2.5251e-01,  1.0883e+00,\n",
            "           1.5243e+00,  1.1334e+00, -1.0611e+00],\n",
            "         [ 5.2936e-01, -1.7029e+00,  1.3953e+00, -9.0505e-01,  1.6967e-01,\n",
            "           4.8606e-01, -1.7896e+00,  1.6764e+00],\n",
            "         [ 6.0310e-01,  1.5112e+00, -1.2372e+00,  1.2211e+00, -1.2832e+00,\n",
            "           2.2012e+00,  7.1705e-01, -2.7040e-01],\n",
            "         [ 1.0123e-01,  1.4577e-01,  1.4984e+00, -1.9334e-01, -2.0535e+00,\n",
            "          -8.6719e-01, -6.0740e-01, -3.9824e-01]],\n",
            "\n",
            "        [[-7.5221e-01, -5.7703e-01, -5.5734e-01, -2.3247e-01,  1.6208e+00,\n",
            "          -1.6109e-01,  2.5744e-01, -2.3659e+00],\n",
            "         [ 8.1639e-01,  1.5813e+00,  8.8068e-01,  2.9590e+00,  5.3022e-01,\n",
            "          -5.6460e-01, -9.4057e-01,  5.0043e-01],\n",
            "         [-9.2556e-01,  1.9240e-01, -1.3680e+00, -2.6650e+00, -1.3611e+00,\n",
            "          -4.9122e-01,  1.7715e-01, -4.5197e-01],\n",
            "         [ 4.8305e-01,  9.0721e-01,  4.1073e+00, -1.3081e-01,  9.3699e-02,\n",
            "           1.6234e+00, -7.0108e-01,  1.2656e+00],\n",
            "         [-1.5912e-01, -8.4026e-02,  1.0453e+00,  7.5124e-01, -6.2256e-01,\n",
            "           3.3297e-01,  2.9267e-01,  1.3339e+00],\n",
            "         [ 3.2395e-01, -1.0399e+00,  1.0621e-01,  7.2030e-02, -2.1072e-01,\n",
            "          -6.3703e-01,  3.3119e+00,  2.1802e+00],\n",
            "         [-2.5579e-01, -7.7687e-01,  1.7653e+00, -1.0296e-01, -3.8135e-01,\n",
            "           2.9460e+00, -3.5978e-01, -1.4807e-01],\n",
            "         [-9.6694e-02, -2.6152e-01, -9.7379e-02,  7.1471e-01,  7.0450e-01,\n",
            "           8.8327e-01, -1.2414e+00, -1.3206e+00]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "\n",
            "\n",
            "after masking\n",
            "tensor([[[-0.3252,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 0.2235,  0.3709,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [-0.3543, -1.0413,  1.5438,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 0.2117, -0.2984,  0.6092, -1.4986,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [-0.3267, -1.9602,  0.2910, -0.7867, -1.3949,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 0.1729, -0.4232, -0.5189,  1.1365,  0.2216,  0.0293,    -inf,\n",
            "             -inf],\n",
            "         [-0.1812,  0.9526, -2.1602,  0.1045, -0.5025,  0.6011,  4.0828,\n",
            "             -inf],\n",
            "         [-0.7462, -0.2503,  1.1514, -0.1000, -1.8712,  0.4234, -1.0692,\n",
            "           0.1532]],\n",
            "\n",
            "        [[-0.5593,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 0.0944, -0.7217,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 1.0768, -0.2834, -0.2440,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 1.1073,  0.1065,  0.1677, -2.1446,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 0.2163,  1.2713, -0.5889, -1.5853, -0.8781,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [-0.7336,  0.8607,  1.4868, -0.2133, -0.9378, -1.9511,    -inf,\n",
            "             -inf],\n",
            "         [ 0.8986, -1.2177,  0.4100,  0.5089, -0.4805,  0.2242,  1.2294,\n",
            "             -inf],\n",
            "         [ 0.1279, -0.6858,  0.3369, -0.3786, -0.9618,  0.4914,  0.4895,\n",
            "           1.8815]],\n",
            "\n",
            "        [[-1.9182,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 1.5381, -0.3029,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 0.2356, -1.2836,  0.5439,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [-0.9061,  0.5460, -1.3593, -0.1624,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 0.1186,  1.7941,  0.0534, -0.2525,  1.0883,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 0.5294, -1.7029,  1.3953, -0.9051,  0.1697,  0.4861,    -inf,\n",
            "             -inf],\n",
            "         [ 0.6031,  1.5112, -1.2372,  1.2211, -1.2832,  2.2012,  0.7170,\n",
            "             -inf],\n",
            "         [ 0.1012,  0.1458,  1.4984, -0.1933, -2.0535, -0.8672, -0.6074,\n",
            "          -0.3982]],\n",
            "\n",
            "        [[-0.7522,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 0.8164,  1.5813,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [-0.9256,  0.1924, -1.3680,    -inf,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 0.4831,  0.9072,  4.1073, -0.1308,    -inf,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [-0.1591, -0.0840,  1.0453,  0.7512, -0.6226,    -inf,    -inf,\n",
            "             -inf],\n",
            "         [ 0.3240, -1.0399,  0.1062,  0.0720, -0.2107, -0.6370,    -inf,\n",
            "             -inf],\n",
            "         [-0.2558, -0.7769,  1.7653, -0.1030, -0.3814,  2.9460, -0.3598,\n",
            "             -inf],\n",
            "         [-0.0967, -0.2615, -0.0974,  0.7147,  0.7045,  0.8833, -1.2414,\n",
            "          -1.3206]]], grad_fn=<MaskedFillBackward0>)\n",
            "\n",
            "\n",
            "after softmax\n",
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4632, 0.5368, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1223, 0.0615, 0.8162, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3059, 0.1837, 0.4552, 0.0553, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2485, 0.0485, 0.4608, 0.1568, 0.0854, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1518, 0.0836, 0.0760, 0.3978, 0.1594, 0.1315, 0.0000, 0.0000],\n",
            "         [0.0126, 0.0390, 0.0017, 0.0167, 0.0091, 0.0275, 0.8933, 0.0000],\n",
            "         [0.0557, 0.0915, 0.3716, 0.1063, 0.0181, 0.1794, 0.0403, 0.1370]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.6934, 0.3066, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.6564, 0.1684, 0.1752, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5565, 0.2045, 0.2175, 0.0215, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2075, 0.5960, 0.0928, 0.0342, 0.0695, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0558, 0.2747, 0.5137, 0.0938, 0.0455, 0.0165, 0.0000, 0.0000],\n",
            "         [0.2191, 0.0264, 0.1344, 0.1484, 0.0552, 0.1116, 0.3050, 0.0000],\n",
            "         [0.0815, 0.0361, 0.1005, 0.0491, 0.0274, 0.1173, 0.1171, 0.4709]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.8631, 0.1369, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3876, 0.0848, 0.5276, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1248, 0.5333, 0.0793, 0.2626, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0943, 0.5036, 0.0883, 0.0651, 0.2487, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1859, 0.0199, 0.4420, 0.0443, 0.1298, 0.1781, 0.0000, 0.0000],\n",
            "         [0.0854, 0.2118, 0.0136, 0.1584, 0.0129, 0.4222, 0.0957, 0.0000],\n",
            "         [0.1186, 0.1240, 0.4798, 0.0884, 0.0138, 0.0450, 0.0584, 0.0720]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3176, 0.6824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2127, 0.6506, 0.1367, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0247, 0.0377, 0.9243, 0.0133, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1173, 0.1264, 0.3911, 0.2915, 0.0738, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2628, 0.0672, 0.2113, 0.2042, 0.1539, 0.1005, 0.0000, 0.0000],\n",
            "         [0.0273, 0.0162, 0.2058, 0.0318, 0.0241, 0.6703, 0.0246, 0.0000],\n",
            "         [0.0943, 0.0800, 0.0942, 0.2123, 0.2101, 0.2513, 0.0300, 0.0277]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "\n",
            "after wei @ v\n",
            "tensor([[[ 1.1721e-02,  9.4438e-02, -1.2803e-01, -3.9398e-01, -1.6363e-01,\n",
            "          -6.2380e-02, -2.7520e-01,  2.3756e-01,  3.6904e-01, -4.9062e-01,\n",
            "          -2.1050e-01, -5.5707e-01, -4.0623e-01, -7.5812e-01,  4.9140e-01,\n",
            "          -1.5930e-01],\n",
            "         [ 2.0548e-02, -4.2772e-01,  4.5054e-01, -1.1677e-01, -1.3426e-02,\n",
            "           6.2138e-02, -6.9457e-02, -3.3171e-01,  3.1060e-01, -4.5908e-01,\n",
            "          -5.5291e-01, -3.7021e-01,  1.0497e-05, -5.5108e-01,  4.7016e-01,\n",
            "          -2.7704e-01],\n",
            "         [ 2.8423e-01, -1.0247e+00,  2.5345e-01,  1.0672e+00, -1.1143e-01,\n",
            "           4.5461e-01, -2.5920e-02,  2.7992e-01,  6.8098e-01, -9.1459e-01,\n",
            "          -2.1828e-01, -2.0145e-01,  1.5173e+00,  2.8907e-01,  4.2505e-01,\n",
            "          -1.0335e+00],\n",
            "         [ 1.6952e-01, -6.4382e-01,  2.8448e-01,  5.2976e-01, -5.7529e-02,\n",
            "           2.6788e-01, -1.0105e-01,  8.5455e-02,  5.0596e-01, -6.4470e-01,\n",
            "          -2.8199e-01, -2.0767e-01,  8.0401e-01, -8.6399e-02,  4.1221e-01,\n",
            "          -6.8176e-01],\n",
            "         [ 1.7129e-01, -4.8702e-01,  2.4865e-01,  5.3149e-01,  9.7424e-03,\n",
            "           3.1109e-01, -2.0922e-01,  2.0056e-01,  3.9823e-01, -4.2514e-01,\n",
            "          -8.0857e-02, -4.7897e-02,  7.4966e-01, -3.0819e-02,  3.2184e-01,\n",
            "          -6.9005e-01],\n",
            "         [ 3.1139e-02,  6.1479e-02,  3.1889e-01,  8.2992e-02,  2.0242e-01,\n",
            "           1.8059e-01, -3.0148e-01, -5.1874e-02,  4.6010e-02,  1.7668e-01,\n",
            "           8.9414e-02,  3.6562e-01, -1.6566e-03, -2.3720e-01,  1.3715e-01,\n",
            "          -3.0582e-01],\n",
            "         [-7.2087e-01,  6.2732e-01, -4.5573e-01, -4.8989e-01,  5.9636e-01,\n",
            "          -2.0084e-01,  7.1981e-02,  3.7702e-01, -1.2748e-01, -4.4080e-01,\n",
            "           2.7786e-01, -3.1351e-01, -1.1530e-01, -3.2338e-01, -2.4501e-01,\n",
            "          -6.2713e-02],\n",
            "         [ 1.4880e-01, -4.6489e-01,  1.5142e-01,  7.7275e-01,  4.0907e-02,\n",
            "           8.4112e-02,  1.3310e-01,  5.6273e-02,  3.1233e-01, -5.1640e-01,\n",
            "          -1.1074e-01, -1.6866e-03,  6.4309e-01,  1.7117e-01,  2.1559e-01,\n",
            "          -6.8550e-01]],\n",
            "\n",
            "        [[-1.7473e-01, -8.8103e-03,  2.2263e-01,  1.0734e+00,  1.9960e-01,\n",
            "           9.7140e-01, -4.1469e-01,  7.8292e-01, -1.6442e-01, -6.3626e-01,\n",
            "          -1.0740e+00,  6.8252e-01,  1.6249e-01,  4.1906e-01, -1.0401e+00,\n",
            "          -3.8946e-01],\n",
            "         [-1.7033e-01, -8.0314e-02,  5.7360e-02,  6.4469e-01,  1.3046e-01,\n",
            "           9.3629e-01, -1.9639e-01,  5.8906e-01,  6.9054e-03, -3.9656e-01,\n",
            "          -7.7515e-01,  6.0724e-01, -2.3946e-01,  3.4187e-01, -6.7404e-01,\n",
            "           1.6509e-01],\n",
            "         [-1.4233e-01, -5.7419e-02,  4.8167e-02,  5.7422e-01,  2.1331e-01,\n",
            "           9.0369e-01, -2.1834e-01,  6.3134e-01, -6.9164e-02, -4.3296e-01,\n",
            "          -6.5858e-01,  3.2736e-01,  6.3083e-02,  3.4031e-01, -6.0656e-01,\n",
            "          -9.2060e-02],\n",
            "         [-1.4377e-01, -6.5311e-02, -1.2874e-03,  4.3499e-01,  2.2604e-01,\n",
            "           8.6784e-01, -1.5640e-01,  5.8041e-01, -6.1442e-02, -3.8026e-01,\n",
            "          -5.4435e-01,  2.1494e-01,  7.0317e-02,  3.1754e-01, -4.5842e-01,\n",
            "          -1.8080e-02],\n",
            "         [-1.6596e-01, -7.5554e-02, -4.5981e-02, -4.4031e-02,  5.2446e-02,\n",
            "           7.9629e-01,  1.0075e-01,  3.2556e-01,  1.5070e-01, -1.5844e-01,\n",
            "          -2.4240e-01,  2.7984e-01, -5.1621e-01,  1.8766e-01, -4.0749e-02,\n",
            "           7.1675e-01],\n",
            "         [-1.1017e-01, -2.1437e-02, -1.2590e-01, -2.9057e-01,  2.6542e-01,\n",
            "           6.4506e-01,  1.1590e-01,  3.5788e-01, -5.0716e-02, -1.9303e-01,\n",
            "           8.0122e-02, -4.3992e-01,  2.6022e-01,  1.6022e-01,  2.4511e-01,\n",
            "           1.5198e-01],\n",
            "         [-1.5055e-01,  1.7324e-01,  1.2568e-01, -1.2925e-01,  5.4723e-02,\n",
            "           3.2681e-01, -2.9812e-02,  2.6382e-01, -2.4971e-01, -1.0647e-01,\n",
            "          -1.8602e-01,  1.4928e-02,  2.7609e-01,  3.5278e-01, -6.6242e-02,\n",
            "           1.9295e-01],\n",
            "         [-9.0993e-02, -2.5999e-01, -4.2282e-01,  9.8053e-02,  3.5446e-02,\n",
            "          -1.5389e-01, -1.5121e-01,  4.7297e-01,  2.3086e-01,  2.2838e-01,\n",
            "          -2.8833e-01,  6.5715e-01, -3.5087e-01, -8.2030e-02, -9.1757e-02,\n",
            "           2.7209e-01]],\n",
            "\n",
            "        [[-1.2669e-01, -4.4611e-01,  2.0433e-01,  4.8616e-01,  7.2671e-01,\n",
            "           7.4524e-01, -5.9695e-01,  4.1749e-01, -3.8929e-01, -1.3837e+00,\n",
            "           2.5755e-01, -5.7826e-02,  2.0374e-01,  7.9986e-02, -2.8709e-01,\n",
            "           1.4177e+00],\n",
            "         [-1.3982e-01, -3.9909e-01,  7.9349e-02,  4.7610e-01,  6.8850e-01,\n",
            "           7.0223e-01, -4.9374e-01,  4.5150e-01, -3.2440e-01, -1.0831e+00,\n",
            "           2.0725e-01,  1.6306e-02,  2.1333e-01, -3.2028e-02, -1.4761e-01,\n",
            "           1.1138e+00],\n",
            "         [ 2.0838e-01, -1.0471e-01,  3.1429e-02,  1.7023e-01,  2.3206e-01,\n",
            "           4.5690e-01, -2.4891e-02,  2.9588e-01,  9.6126e-02, -6.3524e-01,\n",
            "          -1.8685e-01, -2.0594e-01, -1.0283e-02, -1.2240e-01, -3.9811e-01,\n",
            "           3.1883e-01],\n",
            "         [-2.8769e-02, -2.8314e-01, -3.9423e-01,  5.6697e-01,  4.8234e-01,\n",
            "           1.2971e-01, -2.5332e-02,  4.0747e-01, -4.2681e-02,  4.0816e-01,\n",
            "           1.3085e-01,  2.2660e-01, -9.2460e-03, -3.3532e-01,  1.5128e-01,\n",
            "          -3.2658e-01],\n",
            "         [-2.0547e-01, -2.7799e-01, -5.5093e-01,  3.1345e-01,  3.2599e-01,\n",
            "           2.2316e-01,  5.0027e-01,  2.4987e-01,  7.2823e-02,  3.6525e-01,\n",
            "          -2.5813e-01,  2.5694e-01,  7.3130e-02, -3.3047e-01,  4.1862e-01,\n",
            "          -1.3958e-01],\n",
            "         [ 8.2810e-02, -1.1113e-01,  5.0870e-02,  1.3009e-01,  2.6038e-01,\n",
            "           1.9212e-01,  3.1921e-01,  2.8777e-01,  1.9739e-01, -3.7430e-01,\n",
            "          -2.0643e-01, -1.2796e-01,  1.2569e-01, -1.1231e-01, -1.8088e-01,\n",
            "           6.1388e-02],\n",
            "         [-2.0533e-01, -1.9960e-01,  1.2605e-01,  3.7967e-01,  7.0187e-01,\n",
            "          -4.3129e-02,  4.2828e-02,  6.8077e-01,  4.8236e-02,  1.0286e-01,\n",
            "           2.4868e-01,  1.4300e-01,  4.6007e-01, -2.1904e-01,  3.3147e-01,\n",
            "          -4.3723e-01],\n",
            "         [ 1.6720e-01, -7.7741e-02, -4.9433e-02,  1.8975e-01,  1.8594e-01,\n",
            "           1.4782e-01,  1.7983e-01,  2.6819e-01,  1.6318e-01, -9.2096e-02,\n",
            "          -1.5869e-01, -1.1103e-01, -7.7743e-02, -1.0561e-01, -2.6204e-01,\n",
            "          -9.9968e-02]],\n",
            "\n",
            "        [[ 8.9567e-02,  4.4859e-01, -1.9846e-01, -6.2503e-03,  6.3139e-02,\n",
            "          -9.7654e-01,  1.4172e+00,  2.0936e-01, -6.4711e-02, -5.3950e-02,\n",
            "          -2.4020e-02, -1.7777e-01, -2.6221e-01,  5.8682e-01,  5.9224e-01,\n",
            "          -4.4683e-01],\n",
            "         [ 3.2869e-01, -9.0885e-02,  1.7188e-01, -5.9139e-03,  3.8060e-01,\n",
            "          -7.4950e-01,  3.7490e-01, -6.3579e-02,  3.2343e-02,  3.9540e-01,\n",
            "          -2.5611e-01, -5.7788e-01, -7.6447e-01,  6.8904e-01,  5.0043e-02,\n",
            "          -1.0846e-01],\n",
            "         [ 2.3374e-01, -1.8552e-01,  1.8550e-01, -5.0158e-02,  2.7748e-01,\n",
            "          -5.5087e-01,  1.8879e-01, -1.1255e-01,  9.9391e-02,  3.9712e-01,\n",
            "          -3.2582e-01, -6.3805e-01, -4.4502e-01,  5.1344e-01,  1.4480e-01,\n",
            "          -2.9535e-01],\n",
            "         [-4.5191e-01, -3.8807e-01,  3.2695e-02, -3.1856e-01, -5.1447e-01,\n",
            "           4.6741e-01, -2.3587e-01, -2.3331e-01,  4.1269e-01,  1.4287e-01,\n",
            "          -5.8431e-01, -7.3775e-01,  1.7206e+00, -5.7277e-01,  1.0262e+00,\n",
            "          -1.5780e+00],\n",
            "         [ 1.4898e-01,  3.4938e-02, -8.1515e-03, -3.7887e-01, -9.2333e-02,\n",
            "           6.1398e-02,  2.8764e-01, -2.4570e-01, -7.8526e-02,  5.0016e-01,\n",
            "          -3.2633e-01, -5.2916e-01,  6.6918e-01, -1.9733e-01,  4.7251e-01,\n",
            "          -6.6268e-01],\n",
            "         [ 1.7114e-01,  2.0725e-01, -5.0817e-02, -8.1673e-02, -7.2482e-02,\n",
            "          -1.4927e-01,  4.4152e-01, -4.3841e-02, -1.0864e-01,  2.0164e-01,\n",
            "          -1.0381e-01, -2.7746e-01,  3.6467e-01, -9.5375e-02,  2.2976e-01,\n",
            "          -5.3684e-01],\n",
            "         [ 1.5943e-01,  4.6346e-01,  1.8847e-01,  6.2784e-01, -3.2018e-01,\n",
            "           4.4557e-01, -4.3952e-01,  2.8668e-01, -1.9475e-01, -6.4815e-01,\n",
            "           1.4493e-01, -1.2347e-01,  8.5543e-01, -2.1258e-01, -3.5161e-01,\n",
            "          -1.1171e+00],\n",
            "         [ 2.4286e-01,  2.8774e-01, -6.0470e-03,  1.2309e-01, -5.2436e-02,\n",
            "           4.3537e-02,  1.3225e-01,  3.1955e-02, -1.8322e-01,  3.7275e-02,\n",
            "           6.7108e-02, -1.0563e-01,  3.1699e-01, -2.0177e-01, -1.2227e-01,\n",
            "          -4.3151e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now `wei` has different values for each batch, since each batch has different tokens... so we know that `wei` is data dependent.\n",
        "\n",
        "- query --> heres what im interested in\n",
        "-  key --> heres what i have\n",
        "- value --> if u find anything interesting, heres what i will communicate to u\n",
        "\n",
        "value is what gets aggregated\n",
        "\n"
      ],
      "metadata": {
        "id": "UUwyHNiXjsWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = \"blue\">*for classification (e.g sentiment analysis), we dont need to mask because we can use all future and past information*\n",
        "\n",
        "**in encoder attention:** no masking --- all tokens are able to communicate\n",
        "- the input of an encoder would be the entire sequence at once\n",
        "\n",
        "**in decoder attention:** masking --- future tokens cannot communicate with past\n",
        "- the input of a decoder would be using the sequence in time steps --> so then the target of the decoder is also the sequence (next token prediction)\n",
        "\n",
        "- **self-attention:** key, query, and values come from same tokens\n",
        "\n",
        "- **cross-attention:** key, values come from different tokens (ex: for encoder) and queries come from decoder input tokens ---> think about in translation\n",
        "\n",
        "- **scaled-attention:** if u scale/divide the attention by the sqrt(`head_size`) (dimension of key, value. query) then the variance of the attention will be 1 again. since it will feed into softmax, it is important for the attention to be scaled so that softmax doesnt cause a dying token/neuron type problem where highest values dominant too much\n"
      ],
      "metadata": {
        "id": "iF0VEjw69Fxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9JHzsinidyQ",
        "outputId": "c5c5fc50-94e4-492d-80dc-29ef258bcb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4632, 0.5368, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1223, 0.0615, 0.8162, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3059, 0.1837, 0.4552, 0.0553, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2485, 0.0485, 0.4608, 0.1568, 0.0854, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1518, 0.0836, 0.0760, 0.3978, 0.1594, 0.1315, 0.0000, 0.0000],\n",
              "         [0.0126, 0.0390, 0.0017, 0.0167, 0.0091, 0.0275, 0.8933, 0.0000],\n",
              "         [0.0557, 0.0915, 0.3716, 0.1063, 0.0181, 0.1794, 0.0403, 0.1370]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6934, 0.3066, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6564, 0.1684, 0.1752, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5565, 0.2045, 0.2175, 0.0215, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2075, 0.5960, 0.0928, 0.0342, 0.0695, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0558, 0.2747, 0.5137, 0.0938, 0.0455, 0.0165, 0.0000, 0.0000],\n",
              "         [0.2191, 0.0264, 0.1344, 0.1484, 0.0552, 0.1116, 0.3050, 0.0000],\n",
              "         [0.0815, 0.0361, 0.1005, 0.0491, 0.0274, 0.1173, 0.1171, 0.4709]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8631, 0.1369, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3876, 0.0848, 0.5276, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1248, 0.5333, 0.0793, 0.2626, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0943, 0.5036, 0.0883, 0.0651, 0.2487, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1859, 0.0199, 0.4420, 0.0443, 0.1298, 0.1781, 0.0000, 0.0000],\n",
              "         [0.0854, 0.2118, 0.0136, 0.1584, 0.0129, 0.4222, 0.0957, 0.0000],\n",
              "         [0.1186, 0.1240, 0.4798, 0.0884, 0.0138, 0.0450, 0.0584, 0.0720]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.3176, 0.6824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2127, 0.6506, 0.1367, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0247, 0.0377, 0.9243, 0.0133, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1173, 0.1264, 0.3911, 0.2915, 0.0738, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2628, 0.0672, 0.2113, 0.2042, 0.1539, 0.1005, 0.0000, 0.0000],\n",
              "         [0.0273, 0.0162, 0.2058, 0.0318, 0.0241, 0.6703, 0.0246, 0.0000],\n",
              "         [0.0943, 0.0800, 0.0942, 0.2123, 0.2101, 0.2513, 0.0300, 0.0277]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updated Script"
      ],
      "metadata": {
        "id": "2n4Jva7cPLzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- no longer passing in `vocab_size` for the model methods because we made it as a global variable in the beginning"
      ],
      "metadata": {
        "id": "SDQRdSIJT-U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65-wBR5QPPbj",
        "outputId": "44f56bd8-9b37-4e5c-a862-3d155f2a39da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-15 20:47:55--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-01-15 20:47:56 (110 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5091, val loss 2.5058\n",
            "step 300: train loss 2.4197, val loss 2.4336\n",
            "step 400: train loss 2.3501, val loss 2.3562\n",
            "step 500: train loss 2.2963, val loss 2.3125\n",
            "step 600: train loss 2.2407, val loss 2.2496\n",
            "step 700: train loss 2.2054, val loss 2.2187\n",
            "step 800: train loss 2.1633, val loss 2.1866\n",
            "step 900: train loss 2.1241, val loss 2.1504\n",
            "step 1000: train loss 2.1036, val loss 2.1306\n",
            "step 1100: train loss 2.0698, val loss 2.1180\n",
            "step 1200: train loss 2.0380, val loss 2.0791\n",
            "step 1300: train loss 2.0248, val loss 2.0634\n",
            "step 1400: train loss 1.9926, val loss 2.0359\n",
            "step 1500: train loss 1.9697, val loss 2.0287\n",
            "step 1600: train loss 1.9627, val loss 2.0477\n",
            "step 1700: train loss 1.9403, val loss 2.0115\n",
            "step 1800: train loss 1.9090, val loss 1.9941\n",
            "step 1900: train loss 1.9092, val loss 1.9858\n",
            "step 2000: train loss 1.8847, val loss 1.9925\n",
            "step 2100: train loss 1.8724, val loss 1.9757\n",
            "step 2200: train loss 1.8580, val loss 1.9594\n",
            "step 2300: train loss 1.8560, val loss 1.9537\n",
            "step 2400: train loss 1.8412, val loss 1.9427\n",
            "step 2500: train loss 1.8141, val loss 1.9402\n",
            "step 2600: train loss 1.8292, val loss 1.9397\n",
            "step 2700: train loss 1.8116, val loss 1.9322\n",
            "step 2800: train loss 1.8032, val loss 1.9218\n",
            "step 2900: train loss 1.8022, val loss 1.9285\n",
            "step 3000: train loss 1.7955, val loss 1.9195\n",
            "step 3100: train loss 1.7672, val loss 1.9192\n",
            "step 3200: train loss 1.7568, val loss 1.9138\n",
            "step 3300: train loss 1.7551, val loss 1.9059\n",
            "step 3400: train loss 1.7549, val loss 1.8945\n",
            "step 3500: train loss 1.7383, val loss 1.8956\n",
            "step 3600: train loss 1.7242, val loss 1.8868\n",
            "step 3700: train loss 1.7273, val loss 1.8822\n",
            "step 3800: train loss 1.7176, val loss 1.8923\n",
            "step 3900: train loss 1.7219, val loss 1.8750\n",
            "step 4000: train loss 1.7131, val loss 1.8603\n",
            "step 4100: train loss 1.7105, val loss 1.8777\n",
            "step 4200: train loss 1.7033, val loss 1.8675\n",
            "step 4300: train loss 1.7038, val loss 1.8556\n",
            "step 4400: train loss 1.7057, val loss 1.8643\n",
            "step 4500: train loss 1.6875, val loss 1.8528\n",
            "step 4600: train loss 1.6887, val loss 1.8405\n",
            "step 4700: train loss 1.6834, val loss 1.8501\n",
            "step 4800: train loss 1.6675, val loss 1.8437\n",
            "step 4900: train loss 1.6684, val loss 1.8407\n",
            "step 4999: train loss 1.6645, val loss 1.8286\n",
            "\n",
            "\n",
            "KING RICHARD II:\n",
            "Shal lifest made to bub, to take Our my dagatants:\n",
            "Whith foul his vetward that a endrer, my fears' to zorm heavens,\n",
            "Oof it heart my would but\n",
            "With ensengmin latest in ov the doest not.\n",
            "\n",
            "WARWICK:\n",
            "Welll now, and thus quechiry: there's speak you love.\n",
            "In Bodiet, and whom the sclittle\n",
            "Enout-now what evily well most rive with is compon to the me\n",
            "Town danters, If so;\n",
            "Ange to shall do aleous, for dear?\n",
            "\n",
            "KING HENRY VI:\n",
            "Hark, but a\n",
            "ards bring Edward?\n",
            "\n",
            "GROKE:\n",
            "As is no Rurnts I am you! who neet.\n",
            "Pom mary thou contrantym so a thense.\n",
            "\n",
            "QUEEN VINCENTIO:\n",
            "O, sir, may in God't well ow, whom confessy.\n",
            "Which migh.\n",
            "\n",
            "ARCHILINIUS:\n",
            "Dithul seaze Peed me: very it passce of's cruport;\n",
            "How what make you fear tals: there loves\n",
            "Tunkistren in deed, is xment.\n",
            "\n",
            "CORIONIUS:\n",
            "What comforts me. I with self From the walt I?\n",
            "\n",
            "GRINION:\n",
            "Which ushold.\n",
            "\n",
            "KING HENRY Gindner:\n",
            "Withrief I doot, is onter now.\n",
            "\n",
            "Securming:\n",
            "Intande whose no crown some Eiverely marry sold;\n",
            "For for me watch the\n",
            "our torguet! Goy, know our her and brut what I, I huself as humsell.\n",
            "\n",
            "APTOLYCUM:\n",
            "Laitance and toarth or word\n",
            "As beherefitions so me worting.\n",
            "\n",
            "CORIOLINA:\n",
            "What a wouldds,\n",
            "An but branedy wouldIng my a canity:\n",
            "Was you be any in Becausing watcess the Regreast men is what see would in thas jury your Hrannertandless;\n",
            "As there'erliacter me band frind through he crown, I she love is stay just torment:\n",
            "Slaw you behoth unserving of vonby the post,\n",
            "Whave baste hold; I they nengety may's fries\n",
            "To there's fince, I heave arrow old,\n",
            "Thee best sincess soul be\n",
            "that Lord, as;\n",
            "River thou a-latsteer:\n",
            "Out.\n",
            "\n",
            "PORALLINA:\n",
            "Where but\n",
            "Braight gentle, drieven the know you\n",
            "for that to this mack a rishn. Prawity arm as is infectely,\n",
            "Ah, sinstats o' no, this send; commant to love,\n",
            "Go fly this fathal\n",
            "I cortuns cold, offrong to old, the courtly thee? before a gace.\n",
            "\n",
            "KING RICHARD III:\n",
            "A life he pusict\n",
            "It. Vitters, and were not fanturs, thy promind thy awonse than a braute comforn,\n",
            "Will Roman! you brain shown'd for a dresss me; he heavison!\n",
            "\n",
            "\n",
            "MENE\n"
          ]
        }
      ]
    }
  ]
}