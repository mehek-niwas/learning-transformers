{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOXXpKV6cJ1D/WEkCvoq+1L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehek-niwas/learning_transformer/blob/main/mehek_gpt_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NanoTransformer Notes**\n",
        "*using Andrej Karpathy's video*: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1334s"
      ],
      "metadata": {
        "id": "EtqtfnAwNUM8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SEctH165761",
        "outputId": "40e45a89-ea3c-4dee-ee6a-fcc719ff6be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-08 23:09:55--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-01-08 23:09:55 (27.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3lCF_WB6ODO",
        "outputId": "11f07048-fc8e-4078-cfaf-b1718bea227c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afu3C2G66ZnE",
        "outputId": "47a22fb3-f52e-412d-d5d3-b9297ed0aea1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou0Y0CID6iFw",
        "outputId": "dc86a595-df22-4262-fd26-f5a499608841"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSLDwu_66tos",
        "outputId": "fd3ec5d1-ef97-47a0-a0bc-d0bf4e5a4ecd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "ogK8qKSr8Xb9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"training dataset: \", train_data.shape)\n",
        "print(\"validation dataset: \", val_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8iX7K-T8f06",
        "outputId": "fdf7bf43-68c5-4ea6-ac43-2de63fec9071"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training dataset:  torch.Size([1003854])\n",
            "validation dataset:  torch.Size([111540])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1] # showing first block (batch) --> represents 8 training samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdsaCKwm8bNf",
        "outputId": "5791bcfb-f97a-4704-9b84-0c2e4d4831d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all from the first batch\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K72phDTV-ZGy",
        "outputId": "9201145c-9199-4be1-a8b8-a2714f0905b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"training dataset: \", train_data.shape)\n",
        "print(\"validation dataset: \", val_data.shape)\n",
        "\n",
        "torch.manual_seed(1337) # random number generator for reproducable code\n",
        "\n",
        "batch_size = 4 # ---> THIS IS THE NUMBER OF SEQUENCES PER BATCH\n",
        "block_size = 8 # ---> (MAXIMUM TOKENS PER BATCH PER SEQUENCE) or maximum context length\n",
        "\n",
        "# disclaimer: this code does not ensure block_size seperation in between starting indicies\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "\n",
        "    # if in the train split, we are going to look at the training data\n",
        "    # if in test split, we are going to look at the testing data\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # generate (batch_size) random starting indexes anywhere from 0 to data.length - block_size\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # torch.randint(low=0, high, size) --> where size is tuple (rows, columns) --> for rows, 1 column --> (rows,)\n",
        "\n",
        "    # create batches from random starting indexes\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEKNAi6cB2fK",
        "outputId": "b61245c3-1cd6-4148-d95b-d8ce28eddb41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training dataset:  torch.Size([1003854])\n",
            "validation dataset:  torch.Size([111540])\n",
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# BigramLanguageModel inherits from nn.Module\n",
        "# nn.Module --> base class for all neural network modules. user models should also subclass the Module class\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  # constructor\n",
        "  def __init__(self, vocab_size): #\n",
        "    # ensures the parents class (nn.Module) is properly initialized before adding additional functionality specific to the BigramLanguageModel\n",
        "\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "    # SYNTAX: nn.Embedding(num_embeddings, embedding_dim) --> (size of dictionary of embeddings, size of each embedding vector)\n",
        "    #  --> OUTPUT:  random initialized embedding matrix of vocab_size x vocab_size\n",
        "\n",
        "  def forward(self, inputs, targets=None):\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "\n",
        "    # THIS WORKS FOR THE INPUTS BECAUSE THE INPUTS ARE ALREADY IN INDEX FORM. SO INPUT OF 23 WILL OUTPUT THE EMBEDDING ROW 23\n",
        "    # THIS WOULD NOT WORK IF THE INPUTS WERE STILL IN STRING FORM. --> WE HAD TO SPLIT THEM AND INDEX THEM FIRST(SERVING A LOOKUP TABLE OR WHATEVER METHOD) --> SO NOW WE ARE SWITCHING TO ITS EMBEDDING VECTOR\n",
        "\n",
        "\n",
        "    logits = self.token_embedding_table(inputs) # --> WE NEED TO ARRANGE THIS AS A (B, T, C) # ==== the tensors/embedding vectors related to the idx(s) [from the embedding matrix]\n",
        "    # B = batch size (number of input sequences)  x   T = sequence length  x  C = vocabulary size (logits for each possible token??) --> logits --> interpreted as the scores for the next character in the sequence\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "    # --> SO pytorch expects a B, C, T matrix as the input if the input is multidimensional (btc = 4x8x65)\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C) # making it into a 2 dimensional array!! (was previously B,T,C )\n",
        "      targets = targets.view(B*T) # making into 1 dimensional array!! (was previously B,T)\n",
        "      loss = F.cross_entropy(logits, targets) # loss is the cross entropy of the logits and targets. measures the quality of the logits with respect to the targets\n",
        "      # but targets is not in 4 x 8 x 65 format? it is just in 4 x 8 format?\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx) # --> self(idx) goes to the forward function --> self.function(idx)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) # AUTOREGRESSIVE PART OF THE BIGRAM MODEL!!\n",
        "\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size) # vocab size is the number of different characters in the input\n",
        "logits, loss = m(xb, yb) # same as m.forward(xb, yb) because PyTorch automatically directs m(xb, yb) to the forward method\n",
        "# ---> the scores/logits of a probabilities of each of the 65 character being the next part of the sequence (for every one of the 4 x 8 positions).  ---> 4 x 8 x 65 (imagine a very dense cube)\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "inputs = torch.zeros((1, 1), dtype=torch.long) # zero is kind of like the SOS token since it is the newline character for this project\n",
        "print(decode(m.generate(inputs, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "ew2HauMBU2jy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42a68b3-e1cd-45f9-b263-e939dd42ae62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "think of \"T\" in the BTC as the \"time dimension\" or the \"time step\""
      ],
      "metadata": {
        "id": "7W9CfLCSuf6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = \"blue\"> remember the target in transformers is ALWAYS only 1 token\n",
        "\n",
        "# <font color = \"blue\"> the input in transformers can be a RANGE of tokens up to the CONTEXT WINDOW/SIZE"
      ],
      "metadata": {
        "id": "VMN1ZHlgvLxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PyTorch Embedding Notes**\n",
        "*key component in transformers. used to convert input tokens into continuous representations*\n",
        "\n",
        "\"an embedding layer is a simple lookup table that stores embeddings of a fixed dictionary size\"\n",
        "- the embedding layer = lookup table that maps an index value to a weight matrix of (user-defined) dimension\n",
        "- weight matrix is optimized to produce more useful vectors\n",
        "--> embedding matrix is initialized: `num_embeddings x embedding_dim`\n",
        "- NUM_EMBEDDINGS IS THE DICTIONARY SIZE\n",
        "- given input word or token... (represented by index in the vocabulary)... the index is passed to the embedding layer which looks up the corresponding row in the embedding matrix\n"
      ],
      "metadata": {
        "id": "MPIaR6arMIqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "ekiuFT73xDpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "k7pZxnKZxbFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etagk2TaxHYI",
        "outputId": "89437308-b184-4847-969e-1b99403960c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5580098628997803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJQJg286xJkH",
        "outputId": "ffd3d607-8677-4a96-d311-039e1db19622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bur y ang wod, t se atperares, m:\n",
            "Frerf y g, ncorparthe owns?\n",
            "\n",
            "Tire\n",
            "I trd odeloutes, e couklonsthendf.\n",
            "PESh thovenofieaye kellar we gr mpanou s this?\n",
            "\n",
            "Noflp\n",
            "Ron:\n",
            "OME trdovy\n",
            "IO:\n",
            "Or so waco is. he s!\n",
            "CI I w'd Pordervet ld, temofostrurd tiomy Proute we g gary outhend le, le, yed rat y ay m.\n",
            "\n",
            "nd, yorix$Jave ed thais, mea l herirnonais ire mo par,\n",
            "OFOnealle qu f t atary meee dd il thy fitito-the ssd ovel y hathe, qun s;\n",
            "LO tond t, oullke CIARicousangh'dlke, y lorveloveninde searslly otharomeligse ang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Check\n"
      ],
      "metadata": {
        "id": "z-ZVA1XclsxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi # making sure GPU is running"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JcmW1U2lvXM",
        "outputId": "46de365d-17fd-4972-b17f-7e0467190e73"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan  8 23:10:16 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "numGPUs = len(tf.config.experimental.list_physical_devices('GPU'))\n",
        "\n",
        "print('Num GPUs Available: ', numGPUs)\n",
        "\n",
        "if numGPUs > 0:\n",
        "  print(tf.test.gpu_device_name())\n",
        "  print(device_lib.list_local_devices()[1].physical_device_desc)\n",
        "\n",
        "# check if cuda is available\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f34vfNJFl5IW",
        "outputId": "2906b851-f416-4e11-e588-ad8633dae2e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "/device:GPU:0\n",
            "device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "CUDA is available!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script-Version"
      ],
      "metadata": {
        "id": "dUaw4uYTclz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    # model.eval() is important for disabling droupout layers and adjusting batch normalization behavior\n",
        "    # model.eval() does not automatically disable local gradient computation\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvKN3KjPcksz",
        "outputId": "1f0dc481-c445-4216-ffb1-09323548e608"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-09 03:46:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-01-09 03:46:50 (27.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "step 0: train loss 4.7305, val loss 4.7241\n",
            "step 300: train loss 2.8110, val loss 2.8249\n",
            "step 600: train loss 2.5434, val loss 2.5682\n",
            "step 900: train loss 2.4932, val loss 2.5088\n",
            "step 1200: train loss 2.4863, val loss 2.5035\n",
            "step 1500: train loss 2.4665, val loss 2.4921\n",
            "step 1800: train loss 2.4683, val loss 2.4936\n",
            "step 2100: train loss 2.4696, val loss 2.4846\n",
            "step 2400: train loss 2.4638, val loss 2.4879\n",
            "step 2700: train loss 2.4738, val loss 2.4911\n",
            "\n",
            "od nos CAy go ghanoray t, co haringoudrou clethe k,LARof fr werar,\n",
            "Is fa!\n",
            "\n",
            "\n",
            "Thilemel cia h hmboomyorarifrcitheviPO, tle dst f qur'dig t cof boddo y t o ar pileas h mo wierl t,\n",
            "S:\n",
            "STENENEat I athe thounomy tinrent distesisanimald 3I: eliento ald, avaviconofrisist me Busarend un'soto vat s k,\n",
            "SBRI he the f wendleindd t acoe ts ansu, thy ppr h.QULY:\n",
            "KIIsqu pr odEd ch,\n",
            "APrnes ouse bll owhored miner t ooon'stoume bupromo! fifoveghind hiarnge s.\n",
            "MI aswimy or m, wardd tw'To tee abifewoetsphin sed The a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"The mathematical trick in self-attention\""
      ],
      "metadata": {
        "id": "ttZUEK4xmLWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time (max sequence/context length), channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b42PINVQmPji",
        "outputId": "1fc36ec1-5306-4ea1-8008-5cb37aa62fbb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4tn5EQ5mkvE",
        "outputId": "385abbe3-9459-4cc8-cd63-be8b16dbe19d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we want the 8 tokens (T = 6) to be able to talk to each other\n",
        "\n",
        "the tokens should not be able to talk to tokens in the future\n",
        "\n",
        "information only flows from the previous context to the current time step. and we can't get any information from the future\n",
        "\n",
        "so if i was token 5, id want all the channels from token time steps of 4, 3, 2, and 1 ---> averaging them up --> so that it becomes like a feature vector that sort of summarizes me in the context of my history. just an average will be lossy, so we can do some more special stuff later\n"
      ],
      "metadata": {
        "id": "3ZjyjQjjmzjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An inefficient way to get averages of the previous tokens"
      ],
      "metadata": {
        "id": "kcnSwz1viRBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1] # size: (1, t, C) ---> so technically (t, C) since u can ignore 1 at that point\n",
        "    xbow[b,t] = torch.mean(xprev, 0) # dim=0 means that the mean is computed along the \"first axis\"( --> meaning rows) of the tensor\n",
        "    # so for each column in xprev, the function calculates the mean of all rows (time steps)\n",
        "    # torch.mean(xprev, 0) --> averaging each time step by channel --> (C, )"
      ],
      "metadata": {
        "id": "8INscWeeoB2Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[0])\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ2P531zLdEj",
        "outputId": "1459bcc7-7de6-4971-acb7-def33f488361"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "torch.Size([4, 8, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJyKkZTbL9yk",
        "outputId": "95d170a1-c78b-4323-de3f-a2a83815b0eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write code that prints which inputs in x that a row of x bow is an average of\n",
        "for minibatch in range(B):\n",
        "  print(\"***********************minibatch start******************************\")\n",
        "  for tokenStep in range(T):\n",
        "    print(xbow[minibatch, tokenStep])\n",
        "    print(\"is an average of: \")\n",
        "    print(x[minibatch, :tokenStep+1])\n",
        "    print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YndpA0taMKk6",
        "outputId": "503150b3-0650-484b-b819-2f2a0906aad9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********************minibatch start******************************\n",
            "tensor([ 0.1808, -0.0700])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.0894, -0.4926])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.1490, -0.3199])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.3504, -0.2238])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643]])\n",
            "--------------------------------------------------\n",
            "tensor([0.3525, 0.0545])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.0688, -0.0396])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.0927, -0.0682])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.0341,  0.1332])\n",
            "is an average of: \n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n",
            "--------------------------------------------------\n",
            "***********************minibatch start******************************\n",
            "tensor([ 1.3488, -0.1396])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396]])\n",
            "--------------------------------------------------\n",
            "tensor([0.8173, 0.4127])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.1342,  0.4395])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931]])\n",
            "--------------------------------------------------\n",
            "tensor([0.2711, 0.4774])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910]])\n",
            "--------------------------------------------------\n",
            "tensor([0.2421, 0.0694])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910],\n",
            "        [ 0.1260, -1.5627]])\n",
            "--------------------------------------------------\n",
            "tensor([0.0084, 0.0020])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910],\n",
            "        [ 0.1260, -1.5627],\n",
            "        [-1.1601, -0.3348]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.0712, -0.1128])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910],\n",
            "        [ 0.1260, -1.5627],\n",
            "        [-1.1601, -0.3348],\n",
            "        [ 0.4478, -0.8016]])\n",
            "--------------------------------------------------\n",
            "tensor([0.2527, 0.2149])\n",
            "is an average of: \n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.2858,  0.9651],\n",
            "        [-2.0371,  0.4931],\n",
            "        [ 1.4870,  0.5910],\n",
            "        [ 0.1260, -1.5627],\n",
            "        [-1.1601, -0.3348],\n",
            "        [ 0.4478, -0.8016],\n",
            "        [ 1.5236,  2.5086]])\n",
            "--------------------------------------------------\n",
            "***********************minibatch start******************************\n",
            "tensor([-0.6631, -0.2513])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513]])\n",
            "--------------------------------------------------\n",
            "tensor([ 0.1735, -0.0649])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215]])\n",
            "--------------------------------------------------\n",
            "tensor([0.1685, 0.3348])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.1621,  0.1765])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340],\n",
            "        [-1.1539, -0.2984]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.2312, -0.0436])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340],\n",
            "        [-1.1539, -0.2984],\n",
            "        [-0.5075, -0.9239]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.1015, -0.2855])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340],\n",
            "        [-1.1539, -0.2984],\n",
            "        [-0.5075, -0.9239],\n",
            "        [ 0.5467, -1.4948]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.2593, -0.1630])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340],\n",
            "        [-1.1539, -0.2984],\n",
            "        [-0.5075, -0.9239],\n",
            "        [ 0.5467, -1.4948],\n",
            "        [-1.2057,  0.5718]])\n",
            "--------------------------------------------------\n",
            "tensor([-0.3015, -0.2293])\n",
            "is an average of: \n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 1.0101,  0.1215],\n",
            "        [ 0.1584,  1.1340],\n",
            "        [-1.1539, -0.2984],\n",
            "        [-0.5075, -0.9239],\n",
            "        [ 0.5467, -1.4948],\n",
            "        [-1.2057,  0.5718],\n",
            "        [-0.5974, -0.6937]])\n",
            "--------------------------------------------------\n",
            "***********************minibatch start******************************\n",
            "tensor([ 1.6455, -0.8030])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.4985, -0.5395])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759]])\n",
            "--------------------------------------------------\n",
            "tensor([0.4954, 0.3420])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.0623, -0.1802])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048],\n",
            "        [ 2.7630, -1.7465]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.1401, -0.4462])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048],\n",
            "        [ 2.7630, -1.7465],\n",
            "        [ 1.4516, -1.5103]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.0870, -0.4071])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048],\n",
            "        [ 2.7630, -1.7465],\n",
            "        [ 1.4516, -1.5103],\n",
            "        [ 0.8212, -0.2115]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.0430, -0.1299])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048],\n",
            "        [ 2.7630, -1.7465],\n",
            "        [ 1.4516, -1.5103],\n",
            "        [ 0.8212, -0.2115],\n",
            "        [ 0.7789,  1.5333]])\n",
            "--------------------------------------------------\n",
            "tensor([ 1.1138, -0.1641])\n",
            "is an average of: \n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.3514, -0.2759],\n",
            "        [-1.5108,  2.1048],\n",
            "        [ 2.7630, -1.7465],\n",
            "        [ 1.4516, -1.5103],\n",
            "        [ 0.8212, -0.2115],\n",
            "        [ 0.7789,  1.5333],\n",
            "        [ 1.6097, -0.4032]])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An efficient way to get averages of the previous tokens"
      ],
      "metadata": {
        "id": "3nElV1Axn36S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T,T))\n",
        "print(wei)\n",
        "print(wei.sum(1, keepdim = True))\n",
        "# NORMALIZING THE MATRIX\n",
        "wei = wei / wei.sum(1, keepdim=True) # wei / (summing across columns --> so sum each row) = average of\n",
        "print(wei)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvI8OPIoj8GF",
        "outputId": "05a959a8-ca6f-49bb-f76f-e4d625f87f53"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [4.],\n",
            "        [5.],\n",
            "        [6.],\n",
            "        [7.],\n",
            "        [8.]])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# My method\n",
        "for minibatch in range(B):\n",
        "  currX = x[minibatch]\n",
        "  print(wei @ currX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdB_Ug-dm9Sq",
        "outputId": "e427ed7c-7a68-4e5c-be81-a8f8e0db5a51"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n",
            "tensor([[ 1.3488, -0.1396],\n",
            "        [ 0.8173,  0.4127],\n",
            "        [-0.1342,  0.4395],\n",
            "        [ 0.2711,  0.4774],\n",
            "        [ 0.2421,  0.0694],\n",
            "        [ 0.0084,  0.0020],\n",
            "        [ 0.0712, -0.1128],\n",
            "        [ 0.2527,  0.2149]])\n",
            "tensor([[-0.6631, -0.2513],\n",
            "        [ 0.1735, -0.0649],\n",
            "        [ 0.1685,  0.3348],\n",
            "        [-0.1621,  0.1765],\n",
            "        [-0.2312, -0.0436],\n",
            "        [-0.1015, -0.2855],\n",
            "        [-0.2593, -0.1630],\n",
            "        [-0.3015, -0.2293]])\n",
            "tensor([[ 1.6455, -0.8030],\n",
            "        [ 1.4985, -0.5395],\n",
            "        [ 0.4954,  0.3420],\n",
            "        [ 1.0623, -0.1802],\n",
            "        [ 1.1401, -0.4462],\n",
            "        [ 1.0870, -0.4071],\n",
            "        [ 1.0430, -0.1299],\n",
            "        [ 1.1138, -0.1641]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch method\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "print(xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeoJBd0LoKIp",
        "outputId": "e2951e04-8e2f-46d4-e10a-988a2ab67db6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An even more efficient way to get averages of the previous tokens"
      ],
      "metadata": {
        "id": "JqkzpK9LrIgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T,T))\n",
        "print(tril)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZuHmbPl_hsw",
        "outputId": "6cecb807-ed59-4d40-8acc-b3eefb533faa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # make all elements where tril = 0, --> replace with -inf\n",
        "wei = F.softmax(wei, dim=-1) # dim=-1 --> means take softmax along every single row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdZ0aI_tAn1n",
        "outputId": "afb4e026-970f-409b-beb8-45763db0617c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN0-fMfKMGlv",
        "outputId": "270bff32-0c5d-450f-9aac-4143f8614e6d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### in attention, we use weights similar to the above matrix. the mask will remain, but instead of normalized average weights, the weights will be based on the amount of affinity a token has for the next token prediction.\n",
        "\n",
        "### when you multiply the weights by the token embeddings, you are multiply the weights by each channel to get a token embeddings matrix. so the same weight will be applied all of the channels for a certain sample (feature-vector)\n",
        "\n"
      ],
      "metadata": {
        "id": "Js4L5_vrLrxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHFcVGE5MDJH",
        "outputId": "ad3d9202-2480-4341-e25a-f823e1c5dc91"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.3596, -0.9152],\n",
              "         [ 0.6258,  0.0255],\n",
              "         [ 0.9545,  0.0643],\n",
              "         [ 0.3612,  1.1679],\n",
              "         [-1.3499, -0.5102],\n",
              "         [ 0.2360, -0.2398],\n",
              "         [-0.9211,  1.5433]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.2858,  0.9651],\n",
              "         [-2.0371,  0.4931],\n",
              "         [ 1.4870,  0.5910],\n",
              "         [ 0.1260, -1.5627],\n",
              "         [-1.1601, -0.3348],\n",
              "         [ 0.4478, -0.8016],\n",
              "         [ 1.5236,  2.5086]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 1.0101,  0.1215],\n",
              "         [ 0.1584,  1.1340],\n",
              "         [-1.1539, -0.2984],\n",
              "         [-0.5075, -0.9239],\n",
              "         [ 0.5467, -1.4948],\n",
              "         [-1.2057,  0.5718],\n",
              "         [-0.5974, -0.6937]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.3514, -0.2759],\n",
              "         [-1.5108,  2.1048],\n",
              "         [ 2.7630, -1.7465],\n",
              "         [ 1.4516, -1.5103],\n",
              "         [ 0.8212, -0.2115],\n",
              "         [ 0.7789,  1.5333],\n",
              "         [ 1.6097, -0.4032]]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei @ x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_dhJ3ohAwk7",
        "outputId": "efbcd13d-adcc-47ea-a6c6-13115e795958"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 0.1735, -0.0649],\n",
              "         [ 0.1685,  0.3348],\n",
              "         [-0.1621,  0.1765],\n",
              "         [-0.2312, -0.0436],\n",
              "         [-0.1015, -0.2855],\n",
              "         [-0.2593, -0.1630],\n",
              "         [-0.3015, -0.2293]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.4985, -0.5395],\n",
              "         [ 0.4954,  0.3420],\n",
              "         [ 1.0623, -0.1802],\n",
              "         [ 1.1401, -0.4462],\n",
              "         [ 1.0870, -0.4071],\n",
              "         [ 1.0430, -0.1299],\n",
              "         [ 1.1138, -0.1641]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # since -inf = 0 in softmmax\n",
        "wei = F.softmax(wei, dim=-1) # ---> softmax is a normalization function\n",
        "xbow3 = wei @ x\n",
        "print(xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ei5eEjbrKqT",
        "outputId": "742038bb-c045-4f6e-d1aa-737d48d9eaac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updated Script"
      ],
      "metadata": {
        "id": "2n4Jva7cPLzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- no longer passing in `vocab_size` because we made it as a global variable in the beginning"
      ],
      "metadata": {
        "id": "SDQRdSIJT-U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "embed_dim = 32\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, embed_dim) # each position from 0 to block_size-1 gets its own embedding vector\n",
        "        # ^^ outputs a matrix: block_size x embed_dim --> (T,C)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape # getting the B and T values from the idx shape (idx means inputs)\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        token_embeds = self.token_embedding_table(idx) # (B,T,embed_dim)\n",
        "        pos_embeds = self.position_embedding_table(torch.arrange(T, device=device)) # retrieving indicies of (T,C) from 0 to t-1\n",
        "        x = token_embeds + pos_embeds # --> (B,T,embed_dim) + (T,embed_dim) = (B,T,embed_dim) via broadcasting\n",
        "        logits = self.lm_head(x) # --> output: (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "65-wBR5QPPbj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}